{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your names and students ids here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\UX550VD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\UX550VD\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "# Install spaCy (run in terminal/prompt)\n",
    "import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "# Download spaCy's  'en' Model\n",
    "#!{sys.executable} -m spacy download en\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_json ('../News_Category_Dataset_v2.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical data analysis of a dataframe now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.describe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.groupby('category').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see that many categories are similar.\n",
    "#Let´s merge them. We will merge the more specific ones into the more general ones in case of doubt\n",
    "#the code in the cell below was obtained from https://www.kaggle.com/rmisra/news-category-dataset/discussion/114275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category']=data['category'].replace({\"HEALTHY LIVING\": \"WELLNESS\",\n",
    "\"QUEER VOICES\": \"GROUPS VOICES\",\n",
    "\"BUSINESS\": \"BUSINESS & FINANCES\",\n",
    "\"PARENTS\": \"PARENTING\",\n",
    "\"BLACK VOICES\": \"GROUPS VOICES\",\n",
    "\"THE WORLDPOST\": \"WORLD NEWS\",\n",
    "\"STYLE\": \"STYLE & BEAUTY\",\n",
    "\"GREEN\": \"ENVIRONMENT\",\n",
    "\"TASTE\": \"FOOD & DRINK\",\n",
    "\"WORLDPOST\": \"WORLD NEWS\",\n",
    "\"SCIENCE\": \"SCIENCE & TECH\",\n",
    "\"TECH\": \"SCIENCE & TECH\",\n",
    "\"MONEY\": \"BUSINESS & FINANCES\",\n",
    "\"ARTS\": \"ARTS & CULTURE\",\n",
    "\"COLLEGE\": \"EDUCATION\",\n",
    "\"LATINO VOICES\": \"GROUPS VOICES\",\n",
    "\"CULTURE & ARTS\": \"ARTS & CULTURE\",\n",
    "\"FIFTY\": \"MISCELLANEOUS\",\n",
    "\"GOOD NEWS\": \"MISCELLANEOUS\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.groupby('category').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7299.533828002569"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.groupby('category').size()).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we may observe, the data is still highly imbalanced. Therefore further techniques will have to be applied to\n",
    "make up for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning:\n",
    "\n",
    "We must  apply either lemmatization or stemming to the words.\n",
    "We will try to go for lemmatization first as it depends on the context of the word.\n",
    "\n",
    "We used the following link to guide ourselves https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "\n",
    "//TODO: preproces words to remove apostrophes for example. isn´t -> is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "\n",
    "    # Define the sentence to be lemmatized\n",
    "    text=text.lower() #making everything lowercase\n",
    "    \n",
    "    # Tokenize: Split the sentence into words\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    #print(word_list)\n",
    "\n",
    "    # Lemmatize list of words and join\n",
    "    #lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    #print(lemmatized_output)\n",
    "    \n",
    "   # return lemmatized_output\n",
    "    return word_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['short_description'] = data['short_description'].apply(clean)\n",
    "data['headline'] = data['headline'].apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code from assignment 2 was changed here. We had a framework to start with this way.\n",
    "\n",
    "class FeatureCreator:\n",
    "    def __init__(self):\n",
    "        self.voc = {}\n",
    "        self.id = {}\n",
    "        self.count = 0\n",
    "    \n",
    "    \n",
    "    def createFeatureSet(self, data):\n",
    "        # t1 and t2 are the 2 questions we want to compare\n",
    "        for index, new in data.iterrows():\n",
    "            \n",
    "            for w in new['headline']:\n",
    "                \n",
    "                if w in self.voc:\n",
    "                    self.voc[w] += 1\n",
    "                else:\n",
    "                    \n",
    "                    self.voc[w] = 1\n",
    "                    self.id[w] = self.count\n",
    "                    self.count += 1;\n",
    "                    \n",
    "            for w in new['short_description']:\n",
    "                if w in self.voc:\n",
    "                    self.voc[w] += 1\n",
    "                else:\n",
    "                    self.voc[w] = 1\n",
    "                    self.id[w] = self.count\n",
    "                    self.count += 1;\n",
    "                \n",
    "        \n",
    "\n",
    "class BowFeatureCreator(FeatureCreator):\n",
    "    #still need to change this one\n",
    "     def createFeatures(self,data):\n",
    "        id_len=len(self.id) #we will add this length to the index if we want to access the second question. It is made clear in the implementation\n",
    "\n",
    "        features = dok_matrix((len(data),id_len*2)) #we will have twice the entries. One |VOC| x |2| one for the headline and the other one for the description\n",
    "        label = []\n",
    "        for i in range(len(data)):\n",
    "            for w in data[i][0]:\n",
    "                if w in self.id:\n",
    "                    features[i,self.id[w]] = 1\n",
    "            for w in data[i][1]:\n",
    "                if w in self.id:\n",
    "                    features[i, self.id[w]+ id_len] = 1 #now there is one bow for the headline and another for the description\n",
    "            label.append(data[i][2])\n",
    "        return features,label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate size of the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117873\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features=FeatureCreator()\n",
    "features.createFeatureSet(data)\n",
    "print(features.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89989808, 349124520)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-031b8ce1c5f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model_embeddings.wv.most_similar(positive=[\"korea\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \"\"\"\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song' not present\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-e2636bdbbb7d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-43-e2636bdbbb7d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    .\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "..\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "..\n",
    ".\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "..\n",
    "\n",
    ".\n",
    ".\n",
    ".Ignore from here onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE FROM HERE ONWARDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//we will trin the model once we are done with the cleaning\n",
    "\n",
    "First we split the data such that the proportion of category labels is the same in train and test. Remember, category will be our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, stratify=data['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave that for later, do not pay too much attention for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-c27e5a755d4f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-37-c27e5a755d4f>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    Then you use the class weights during the training process:\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)))\n",
    "\n",
    "Then you use the class weights during the training process:\n",
    "\n",
    "train_history = model.fit( train_dataset, steps_per_epoch=n_steps, class_weight=class_weights )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[1,'headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "for i, row in data.iterrows():\n",
    "    sentences.append(row['headline'])\n",
    "\n",
    "model_embeddings=Word2Vec(vector_size=20, min_count=1)\n",
    "model_embeddings.build_vocab(sentences, progress_per=10000)\n",
    "model_embeddings.train(sentences, total_examples=model_embeddings.corpus_count, epochs=30,report_delay=1)\n",
    "#model_embeddings.build(data['headlines'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings.wv.most_similar(positive=[\"korea\"])\n",
    "model_embeddings.wv.most_similar(positive=[\"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
