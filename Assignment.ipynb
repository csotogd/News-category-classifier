{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your names and students ids here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bmsbu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "# Install spaCy (run in terminal/prompt)\n",
    "import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "# Download spaCy's  'en' Model\n",
    "#!{sys.executable} -m spacy download en\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_json ('News_Category_Dataset_v2.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical data analysis of a dataframe now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200853, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200853 entries, 0 to 200852\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   category           200853 non-null  object        \n",
      " 1   headline           200853 non-null  object        \n",
      " 2   authors            200853 non-null  object        \n",
      " 3   link               200853 non-null  object        \n",
      " 4   short_description  200853 non-null  object        \n",
      " 5   date               200853 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.describe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "ARTS               1509\n",
      "ARTS & CULTURE     1339\n",
      "BLACK VOICES       4528\n",
      "BUSINESS           5937\n",
      "COLLEGE            1144\n",
      "COMEDY             5175\n",
      "CRIME              3405\n",
      "CULTURE & ARTS     1030\n",
      "DIVORCE            3426\n",
      "EDUCATION          1004\n",
      "ENTERTAINMENT     16058\n",
      "ENVIRONMENT        1323\n",
      "FIFTY              1401\n",
      "FOOD & DRINK       6226\n",
      "GOOD NEWS          1398\n",
      "GREEN              2622\n",
      "HEALTHY LIVING     6694\n",
      "HOME & LIVING      4195\n",
      "IMPACT             3459\n",
      "LATINO VOICES      1129\n",
      "MEDIA              2815\n",
      "MONEY              1707\n",
      "PARENTING          8677\n",
      "PARENTS            3955\n",
      "POLITICS          32739\n",
      "QUEER VOICES       6314\n",
      "RELIGION           2556\n",
      "SCIENCE            2178\n",
      "SPORTS             4884\n",
      "STYLE              2254\n",
      "STYLE & BEAUTY     9649\n",
      "TASTE              2096\n",
      "TECH               2082\n",
      "THE WORLDPOST      3664\n",
      "TRAVEL             9887\n",
      "WEDDINGS           3651\n",
      "WEIRD NEWS         2670\n",
      "WELLNESS          17827\n",
      "WOMEN              3490\n",
      "WORLD NEWS         2177\n",
      "WORLDPOST          2579\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.groupby('category').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see that many categories are similar.\n",
    "#Let´s merge them. We will merge the more specific ones into the more general ones in case of doubt\n",
    "#the code in the cell below was obtained from https://www.kaggle.com/rmisra/news-category-dataset/discussion/114275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category']=data['category'].replace({\"HEALTHY LIVING\": \"WELLNESS\",\n",
    "\"QUEER VOICES\": \"GROUPS VOICES\",\n",
    "\"BUSINESS\": \"BUSINESS & FINANCES\",\n",
    "\"PARENTS\": \"PARENTING\",\n",
    "\"BLACK VOICES\": \"GROUPS VOICES\",\n",
    "\"THE WORLDPOST\": \"WORLD NEWS\",\n",
    "\"STYLE\": \"STYLE & BEAUTY\",\n",
    "\"GREEN\": \"ENVIRONMENT\",\n",
    "\"TASTE\": \"FOOD & DRINK\",\n",
    "\"WORLDPOST\": \"WORLD NEWS\",\n",
    "\"SCIENCE\": \"SCIENCE & TECH\",\n",
    "\"TECH\": \"SCIENCE & TECH\",\n",
    "\"MONEY\": \"BUSINESS & FINANCES\",\n",
    "\"ARTS\": \"ARTS & CULTURE\",\n",
    "\"COLLEGE\": \"EDUCATION\",\n",
    "\"LATINO VOICES\": \"GROUPS VOICES\",\n",
    "\"CULTURE & ARTS\": \"ARTS & CULTURE\",\n",
    "\"FIFTY\": \"MISCELLANEOUS\",\n",
    "\"GOOD NEWS\": \"MISCELLANEOUS\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "ARTS & CULTURE          3878\n",
      "BUSINESS & FINANCES     7644\n",
      "COMEDY                  5175\n",
      "CRIME                   3405\n",
      "DIVORCE                 3426\n",
      "EDUCATION               2148\n",
      "ENTERTAINMENT          16058\n",
      "ENVIRONMENT             3945\n",
      "FOOD & DRINK            8322\n",
      "GROUPS VOICES          11971\n",
      "HOME & LIVING           4195\n",
      "IMPACT                  3459\n",
      "MEDIA                   2815\n",
      "MISCELLANEOUS           2799\n",
      "PARENTING              12632\n",
      "POLITICS               32739\n",
      "RELIGION                2556\n",
      "SCIENCE & TECH          4260\n",
      "SPORTS                  4884\n",
      "STYLE & BEAUTY         11903\n",
      "TRAVEL                  9887\n",
      "WEDDINGS                3651\n",
      "WEIRD NEWS              2670\n",
      "WELLNESS               24521\n",
      "WOMEN                   3490\n",
      "WORLD NEWS              8420\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.groupby('category').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7299.533828002569"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.groupby('category').size()).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we may observe, the data is still highly imbalanced: some categories contain about 1000 samples while others contain more than 10000. Therefore further techniques will have to be applied to\n",
    "make up for this\n",
    "(Using K-fold Cross-Validation?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-d290551e9efd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# pip install imblearn (if you don't have imblearn in your system)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conda install -c conda-forge imbalanced-learn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Original categories sizes %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "# import SMOTE module from imblearn library\n",
    "# pip install imblearn (if you don't have imblearn in your system)\n",
    "!conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print('Original categories sizes %s' % data.groupby('category').size())\n",
    "sm = SMOTE(random_state = 2)\n",
    "data_res, y_res = sm.fit_sample(data, data['category'])\n",
    "print('Resampled categories sizes %s' % data_res.groupby('category').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning:\n",
    "\n",
    "We must  apply either lemmatization or stemming to the words.\n",
    "We will try to go for lemmatization first as it depends on the context of the word.\n",
    "\n",
    "We used the following link to guide ourselves https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "\n",
    "//TODO: preproces words to remove apostrophes for example. isn´t -> is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "\n",
    "    # Define the sentence to be lemmatized\n",
    "    text=text.lower() #making everything lowercase\n",
    "    \n",
    "    # Tokenize: Split the sentence into words\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    #print(word_list)\n",
    "\n",
    "    # Lemmatize list of words and join\n",
    "    #lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    #print(lemmatized_output)\n",
    "    \n",
    "   # return lemmatized_output\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['short_description'] = data['short_description'].apply(clean)\n",
    "data['headline'] = data['headline'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODING CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform strings into numerical labels. Remember, if we extend our model to work with NN, \n",
    "then we should transform to one-hot encoding instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARTS & CULTURE', 'BUSINESS & FINANCES', 'COMEDY', 'CRIME', 'DIVORCE', 'EDUCATION', 'ENTERTAINMENT', 'ENVIRONMENT', 'FOOD & DRINK', 'GROUPS VOICES', 'HOME & LIVING', 'IMPACT', 'MEDIA', 'MISCELLANEOUS', 'PARENTING', 'POLITICS', 'RELIGION', 'SCIENCE & TECH', 'SPORTS', 'STYLE & BEAUTY', 'TRAVEL', 'WEDDINGS', 'WEIRD NEWS', 'WELLNESS', 'WOMEN', 'WORLD NEWS']\n",
      "[ 8  9 10]\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['category'])\n",
    "print(list(le.classes_))\n",
    "print(le.transform(['FOOD & DRINK', 'GROUPS VOICES', 'HOME & LIVING']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "      <th>category encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>[there, were, 2, mass, shootings, in, texas, l...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>[she, left, her, husband, ., he, killed, their...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>[will, smith, joins, diplo, and, nicky, jam, f...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>[of, course, it, has, a, song, .]</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>[hugh, grant, marries, for, the, first, time, ...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>[the, actor, and, his, longtime, girlfriend, a...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>[jim, carrey, blasts, 'castrato, ', adam, schi...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>[the, actor, gives, dems, an, ass-kicking, for...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>[julianna, margulies, uses, donald, trump, poo...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>[the, ``, dietland, '', actress, said, using, ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  [there, were, 2, mass, shootings, in, texas, l...   \n",
       "1  ENTERTAINMENT  [will, smith, joins, diplo, and, nicky, jam, f...   \n",
       "2  ENTERTAINMENT  [hugh, grant, marries, for, the, first, time, ...   \n",
       "3  ENTERTAINMENT  [jim, carrey, blasts, 'castrato, ', adam, schi...   \n",
       "4  ENTERTAINMENT  [julianna, margulies, uses, donald, trump, poo...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \\\n",
       "0  [she, left, her, husband, ., he, killed, their... 2018-05-26   \n",
       "1                  [of, course, it, has, a, song, .] 2018-05-26   \n",
       "2  [the, actor, and, his, longtime, girlfriend, a... 2018-05-26   \n",
       "3  [the, actor, gives, dems, an, ass-kicking, for... 2018-05-26   \n",
       "4  [the, ``, dietland, '', actress, said, using, ... 2018-05-26   \n",
       "\n",
       "   category encoded  \n",
       "0                 3  \n",
       "1                 6  \n",
       "2                 6  \n",
       "3                 6  \n",
       "4                 6  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category encoded'] = le.transform(data['category'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "test=data.drop(train.index)\n",
    "\n",
    "train.index= range(len(train))\n",
    "test.reset_index= range(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=160682, step=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200853"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUR MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code from assignment 2 was changed here. We had a framework to start with this way.\n",
    "\n",
    "class FeatureCreator:\n",
    "    def __init__(self):\n",
    "        self.voc = {}\n",
    "        self.id = {}\n",
    "        self.count = 0\n",
    "    \n",
    "    def createFeatureSet(self, data):\n",
    "        # t1 and t2 are the 2 questions we want to compare\n",
    "        for index, new in data.iterrows():\n",
    "            \n",
    "            for w in new['headline']:\n",
    "                if w in self.voc:\n",
    "                    self.voc[w] += 1\n",
    "                else:\n",
    "                    self.voc[w] = 1\n",
    "                    self.id[w] = self.count\n",
    "                    self.count += 1\n",
    "                    \n",
    "            for w in new['short_description']:\n",
    "                if w in self.voc:\n",
    "                    self.voc[w] += 1\n",
    "                else:\n",
    "                    self.voc[w] = 1\n",
    "                    self.id[w] = self.count\n",
    "                    self.count += 1\n",
    "                \n",
    "        \n",
    "class BowFeatureCreator(FeatureCreator): #runs in O(n^2)\n",
    "    \n",
    "     def createFeatures(self,data):\n",
    "        id_len=len(self.id) #we will add this length to the index if we want to access the second question. It is made clear in the implementation\n",
    "\n",
    "        features = dok_matrix((len(data),self.count*2)) #we will have twice the entries. One |VOC| x |2| one for the headline and the other one for the description\n",
    "        print(features.shape)\n",
    "        label = []\n",
    "        i=0\n",
    "        for i, row in data.iterrows():\n",
    "            for w in row['headline']:\n",
    "                if w in self.id:\n",
    "                    features[i,self.id[w]] += 1\n",
    "                    #features[i,self.id[w] + self.count] += 1\n",
    "            for w in row['short_description']:\n",
    "                if w in self.id:\n",
    "                    #features[i, self.id[w]]+= 1 #now there is one bow for the headline and another for the description\n",
    "                    features[i,self.id[w] + self.count] += 1\n",
    "            label.append(row['category encoded'])\n",
    "            \n",
    "            \n",
    "        #we want to make difference independent of the sentences,\n",
    "        #that is, no negative values. We must take absolute value of features\n",
    "        # not the most efficient thing ever here, but it does the job\n",
    "        \"\"\" i=0\n",
    "        for j, row in data.iterrows():\n",
    "            for w in row['headline']:\n",
    "                if w in self.id:\n",
    "                    if features[j,self.id[w]+self.count]<0:\n",
    "                        features[j,self.id[w]+self.count] *= -1\n",
    "            for w in row['short_description']:\n",
    "                if w in self.id:\n",
    "                     if features[j,self.id[w]+self.count]<0:\n",
    "                        features[j,self.id[w] + self.count] *= -1\n",
    "            i+=1.\n",
    "            if i%1000 == 0:\n",
    "                print('iteration: ', i)\"\"\"\n",
    "\n",
    "        #features=self.absoluteValue(features,data)\n",
    "        return features,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate size of the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features=BowFeatureCreator()\n",
    "features.createFeatureSet(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105693\n",
      "105693\n",
      "105693\n",
      "160682\n"
     ]
    }
   ],
   "source": [
    "print(len(features.id))\n",
    "print(len(features.voc))\n",
    "print(features.count)\n",
    "\n",
    "#print(train.shape)\n",
    "print(len(train))\n",
    "\n",
    "for k,v in features.id.items():\n",
    "    if v > len(features.id):\n",
    "        print(k)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160682, 211386)\n",
      "105693\n"
     ]
    }
   ],
   "source": [
    "f,l = features.createFeatures(train)\n",
    "print(features.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(f, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION\n",
    "\n",
    "We need to create many more variations with many different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.16      0.28      3099\n",
      "           1       0.81      0.35      0.49      6234\n",
      "           2       0.93      0.10      0.18      4139\n",
      "           3       0.85      0.31      0.46      2716\n",
      "           4       0.99      0.18      0.31      2762\n",
      "           5       0.90      0.01      0.02      1708\n",
      "           6       0.59      0.83      0.69     12823\n",
      "           7       0.92      0.16      0.27      3154\n",
      "           8       0.90      0.66      0.76      6656\n",
      "           9       0.72      0.52      0.60      9500\n",
      "          10       0.97      0.33      0.49      3337\n",
      "          11       0.96      0.02      0.03      2738\n",
      "          12       0.96      0.04      0.08      2269\n",
      "          13       0.96      0.02      0.04      2267\n",
      "          14       0.60      0.70      0.64     10070\n",
      "          15       0.55      0.95      0.69     26213\n",
      "          16       0.98      0.04      0.08      2022\n",
      "          17       0.96      0.18      0.30      3437\n",
      "          18       0.95      0.37      0.53      3925\n",
      "          19       0.74      0.81      0.77      9574\n",
      "          20       0.75      0.77      0.76      7900\n",
      "          21       0.96      0.29      0.45      2911\n",
      "          22       0.91      0.05      0.10      2076\n",
      "          23       0.45      0.93      0.61     19640\n",
      "          24       0.97      0.03      0.07      2769\n",
      "          25       0.84      0.54      0.66      6743\n",
      "\n",
      "    accuracy                           0.60    160682\n",
      "   macro avg       0.85      0.36      0.40    160682\n",
      "weighted avg       0.72      0.60      0.55    160682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# testf,testl = features.createFeatures(test)\n",
    "predicted_nb = mnb.predict(f)\n",
    "print(classification_report(l, predicted_nb));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE FROM HERE ONWARDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//we will trin the model once we are done with the cleaning\n",
    "\n",
    "First we split the data such that the proportion of category labels is the same in train and test. Remember, category will be our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, stratify=data['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave that for later, do not pay too much attention for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)))\n",
    "\n",
    "Then you use the class weights during the training process:\n",
    "\n",
    "train_history = model.fit( train_dataset, steps_per_epoch=n_steps, class_weight=class_weights )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1,'headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "for i, row in data.iterrows():\n",
    "    sentences.append(row['headline'])\n",
    "\n",
    "model_embeddings=Word2Vec(vector_size=20, min_count=1)\n",
    "model_embeddings.build_vocab(sentences, progress_per=10000)\n",
    "model_embeddings.train(sentences, total_examples=model_embeddings.corpus_count, epochs=30,report_delay=1)\n",
    "#model_embeddings.build(data['headlines'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings.wv.most_similar(positive=[\"korea\"])\n",
    "model_embeddings.wv.most_similar(positive=[\"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
