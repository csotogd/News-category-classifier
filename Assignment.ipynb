{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your names and students ids here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "# from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "# Install spaCy (run in terminal/prompt)\n",
    "import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "# Download spaCy's  'en' Model\n",
    "#!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_json ('News_Category_Dataset_v2.json', lines=True)\n",
    "data = data.drop(columns=['authors', 'link', 'date'])\n",
    "data=data[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical data analysis of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   category           3000 non-null   object\n",
      " 1   headline           3000 non-null   object\n",
      " 2   short_description  3000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 70.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.describe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "category\n",
      "BLACK VOICES      154\n",
      "BUSINESS           21\n",
      "COMEDY            187\n",
      "CRIME              81\n",
      "EDUCATION          15\n",
      "ENTERTAINMENT     677\n",
      "IMPACT             32\n",
      "LATINO VOICES       8\n",
      "MEDIA             113\n",
      "POLITICS         1088\n",
      "QUEER VOICES      150\n",
      "RELIGION           21\n",
      "SCIENCE            15\n",
      "SPORTS             47\n",
      "TECH               30\n",
      "TRAVEL             23\n",
      "WEIRD NEWS         86\n",
      "WOMEN              74\n",
      "WORLD NEWS        178\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.groupby('category').size().size)\n",
    "print(data.groupby('category').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that many categories are similar.\n",
    "Let´s merge them. We will merge the more specific ones into the more general ones in case of doubt\n",
    "the code in the cell below was obtained from https://www.kaggle.com/rmisra/news-category-dataset/discussion/114275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category']=data['category'].replace({\"HEALTHY LIVING\": \"WELLNESS\",\n",
    "\"QUEER VOICES\": \"GROUPS VOICES\",\n",
    "\"BUSINESS\": \"BUSINESS & FINANCES\",\n",
    "\"PARENTS\": \"PARENTING\",\n",
    "\"BLACK VOICES\": \"GROUPS VOICES\",\n",
    "\"THE WORLDPOST\": \"WORLD NEWS\",\n",
    "\"STYLE\": \"STYLE & BEAUTY\",\n",
    "\"GREEN\": \"ENVIRONMENT\",\n",
    "\"TASTE\": \"FOOD & DRINK\",\n",
    "\"WORLDPOST\": \"WORLD NEWS\",\n",
    "\"SCIENCE\": \"SCIENCE & TECH\",\n",
    "\"TECH\": \"SCIENCE & TECH\",\n",
    "\"MONEY\": \"BUSINESS & FINANCES\",\n",
    "\"ARTS\": \"ARTS & CULTURE\",\n",
    "\"COLLEGE\": \"EDUCATION\",\n",
    "\"LATINO VOICES\": \"GROUPS VOICES\",\n",
    "\"CULTURE & ARTS\": \"ARTS & CULTURE\",\n",
    "\"FIFTY\": \"MISCELLANEOUS\",\n",
    "\"GOOD NEWS\": \"MISCELLANEOUS\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "category\n",
      "BUSINESS & FINANCES      21\n",
      "COMEDY                  187\n",
      "CRIME                    81\n",
      "EDUCATION                15\n",
      "ENTERTAINMENT           677\n",
      "GROUPS VOICES           312\n",
      "IMPACT                   32\n",
      "MEDIA                   113\n",
      "POLITICS               1088\n",
      "RELIGION                 21\n",
      "SCIENCE & TECH           45\n",
      "SPORTS                   47\n",
      "TRAVEL                   23\n",
      "WEIRD NEWS               86\n",
      "WOMEN                    74\n",
      "WORLD NEWS              178\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.groupby('category').size().size)\n",
    "print(data.groupby('category').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292.42275333268213"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.groupby('category').size()).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we may observe, the data is still highly imbalanced: some categories contain about 1000 samples while others contain more than 10000. Therefore further techniques will have to be applied to\n",
    "make up for this\n",
    "(Using K-fold Cross-Validation?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW VERSION (with SMOTE and CountVectorizer() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number X_train dataset:  (2100, 1)\n",
      "Number y_train dataset:  (2100,)\n",
      "Number X_test dataset:  (900, 1)\n",
      "Number y_test dataset:  (900,)\n",
      "311     ENTERTAINMENT\n",
      "1025         POLITICS\n",
      "1587    ENTERTAINMENT\n",
      "2941         POLITICS\n",
      "2980         POLITICS\n",
      "            ...      \n",
      "1341    ENTERTAINMENT\n",
      "1612            MEDIA\n",
      "757          POLITICS\n",
      "39             COMEDY\n",
      "1043         POLITICS\n",
      "Name: category, Length: 900, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# y = data['category']\n",
    "y = data['category'].copy()\n",
    "# FIX THIS!! X IS ONLY HEADLINES NOW\n",
    "X = data.drop(columns=['category', 'short_description'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print(\"Number X_train dataset: \", X_train.shape)\n",
    "print(\"Number y_train dataset: \", y_train.shape)\n",
    "print(\"Number X_test dataset: \", X_test.shape)\n",
    "print(\"Number y_test dataset: \", y_test.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge imbalanced-learn\n",
    "# !pip install imblearn\n",
    "# !pip install -U imbalanced-learn\n",
    "# !pip install delayed\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert text data to numeric before applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2     3     4     5     6     7     8     9     ...  5992  \\\n",
      "0        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "3        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
      "2095     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2096     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2097     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2098     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2099     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "      5993  5994  5995  5996  5997  5998  5999  6000  6001  \n",
      "0        0     0     0     0     0     0     0     0     0  \n",
      "1        0     0     0     0     0     0     0     0     0  \n",
      "2        0     0     0     0     0     0     0     0     0  \n",
      "3        0     0     0     0     0     0     0     0     0  \n",
      "4        0     0     0     0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "2095     0     0     0     0     0     0     0     0     0  \n",
      "2096     0     0     0     0     0     0     0     0     0  \n",
      "2097     0     0     0     0     0     0     0     0     0  \n",
      "2098     0     0     0     0     0     0     0     0     0  \n",
      "2099     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[2100 rows x 6002 columns]\n",
      "1732         POLITICS\n",
      "2440            MEDIA\n",
      "1232           COMEDY\n",
      "1081         POLITICS\n",
      "2920         POLITICS\n",
      "            ...      \n",
      "763          POLITICS\n",
      "835             CRIME\n",
      "1653         POLITICS\n",
      "2607            MEDIA\n",
      "2732    GROUPS VOICES\n",
      "Name: category, Length: 2100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train.values.ravel())\n",
    "X_train=vectorizer.transform(X_train.values.ravel())\n",
    "X_test=vectorizer.transform(X_test.values.ravel())\n",
    "X_train=pd.DataFrame(X_train.toarray())\n",
    "X_test=pd.DataFrame(X_test.toarray())\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12016, 6002)\n",
      "(12016,)\n",
      "Original categories sizes category\n",
      "BUSINESS & FINANCES      21\n",
      "COMEDY                  187\n",
      "CRIME                    81\n",
      "EDUCATION                15\n",
      "ENTERTAINMENT           677\n",
      "GROUPS VOICES           312\n",
      "IMPACT                   32\n",
      "MEDIA                   113\n",
      "POLITICS               1088\n",
      "RELIGION                 21\n",
      "SCIENCE & TECH           45\n",
      "SPORTS                   47\n",
      "TRAVEL                   23\n",
      "WEIRD NEWS               86\n",
      "WOMEN                    74\n",
      "WORLD NEWS              178\n",
      "dtype: int64\n",
      "Resampled categories sizes BUSINESS & FINANCES    751\n",
      "COMEDY                 751\n",
      "CRIME                  751\n",
      "EDUCATION              751\n",
      "ENTERTAINMENT          751\n",
      "GROUPS VOICES          751\n",
      "IMPACT                 751\n",
      "MEDIA                  751\n",
      "POLITICS               751\n",
      "RELIGION               751\n",
      "SCIENCE & TECH         751\n",
      "SPORTS                 751\n",
      "TRAVEL                 751\n",
      "WEIRD NEWS             751\n",
      "WOMEN                  751\n",
      "WORLD NEWS             751\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-60803145022d>:9: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  print('Resampled categories sizes %s' % pd.Series(index=y_resampled.values).groupby(level=0).size())\n"
     ]
    }
   ],
   "source": [
    "# import SMOTE module from imblearn library\n",
    "# pip install imblearn (if you don't have imblearn in your system)\n",
    "# !conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE(random_state = 4).fit_resample(X_train, y_train)\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)\n",
    "print('Original categories sizes %s' % data.groupby('category').size())\n",
    "print('Resampled categories sizes %s' % pd.Series(index=y_resampled.values).groupby(level=0).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION\n",
    "\n",
    "We need to create many more variations with many different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 6002)\n",
      "(900,)\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "BUSINESS & FINANCES       0.00      0.00      0.00         2\n",
      "             COMEDY       0.58      0.85      0.69        53\n",
      "              CRIME       0.45      0.38      0.42        26\n",
      "          EDUCATION       0.33      0.33      0.33         6\n",
      "      ENTERTAINMENT       0.64      0.73      0.68       199\n",
      "      GROUPS VOICES       0.53      0.51      0.52        88\n",
      "             IMPACT       0.00      0.00      0.00        14\n",
      "              MEDIA       0.41      0.31      0.35        36\n",
      "           POLITICS       0.77      0.78      0.78       337\n",
      "           RELIGION       0.20      0.20      0.20         5\n",
      "     SCIENCE & TECH       0.00      0.00      0.00        10\n",
      "             SPORTS       0.29      0.20      0.24        10\n",
      "             TRAVEL       0.14      0.17      0.15         6\n",
      "         WEIRD NEWS       0.18      0.06      0.09        33\n",
      "              WOMEN       0.36      0.39      0.37        23\n",
      "         WORLD NEWS       0.67      0.62      0.64        52\n",
      "\n",
      "           accuracy                           0.63       900\n",
      "          macro avg       0.35      0.35      0.34       900\n",
      "       weighted avg       0.61      0.63      0.62       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predicted_nb = mnb.predict(X_test)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(classification_report(y_test, predicted_nb));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW IS THE PREVIOUS VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning:\n",
    "\n",
    "We must  apply either lemmatization or stemming to the words.\n",
    "We will try to go for lemmatization first as it depends on the context of the word.\n",
    "\n",
    "We used the following link to guide ourselves https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "\n",
    "//TODO: preproces words to remove apostrophes for example. isn´t -> is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "spacy_lemmatiser = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "\n",
    "    # Define the sentence to be lemmatized\n",
    "    text=text.lower() #making everything lowercase\n",
    "    \n",
    "    # Tokenize: Split the sentence into words\n",
    "    #word_list = nltk.word_tokenize(text)\n",
    "    #print(word_list)\n",
    "\n",
    "    # Lemmatize list of words and join\n",
    "    #lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    #print(lemmatized_output)\n",
    "    \n",
    "    # return lemmatized_output\n",
    "    #return word_list\n",
    "    \n",
    "    # spacy Lemmatiser with POS tagging\n",
    "    t_l_text = spacy_lemmatiser(text)\n",
    "    lemmas = [token.lemma_ for token in t_l_text]\n",
    "    \n",
    "    lem_stop_text = [tok for tok in lemmas if tok not in stop_words]\n",
    "            \n",
    "    cleaned_text = \" \".join(lem_stop_text) # Takes all the lemmas from text and joins \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['short_description'] = data['short_description'].apply(clean)\n",
    "data['headline'] = data['headline'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODING CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform strings into numerical labels. Remember, if we extend our model to work with NN, \n",
    "then we should transform to one-hot encoding instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['category'])\n",
    "print(list(le.classes_))\n",
    "#print(le.transform(['FOOD & DRINK', 'GROUPS VOICES', 'HOME & LIVING']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category encoded'] = le.transform(data['category'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "train.index= range(len(train))\n",
    "test=data.drop(train.index)\n",
    "test = test.reset_index()\n",
    "print(train.shape)\n",
    "print(train.index)\n",
    "print(test.shape)\n",
    "print(test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, new in test.iterrows():\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUR MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code from assignment 2 was changed here. We had a framework to start with this way.\n",
    "\n",
    "class FeatureCreator:\n",
    "    def __init__(self):\n",
    "        self.voc = {}\n",
    "        self.id = {}\n",
    "        self.count = 0\n",
    "    \n",
    "    def createFeatureSet(self, data):\n",
    "        # t1 and t2 are the 2 questions we want to compare\n",
    "        for index, new in data.iterrows():\n",
    "            \n",
    "            for w in new['headline']:\n",
    "                if w in self.voc:\n",
    "                    self.voc[w] += 1\n",
    "                else:\n",
    "                    self.voc[w] = 1\n",
    "                    self.id[w] = self.count\n",
    "                    self.count += 1\n",
    "                    \n",
    "            for w in new['short_description']:\n",
    "                if w in self.voc:\n",
    "                    self.voc[w] += 1\n",
    "                else:\n",
    "                    self.voc[w] = 1\n",
    "                    self.id[w] = self.count\n",
    "                    self.count += 1\n",
    "                \n",
    "        \n",
    "class BowFeatureCreator(FeatureCreator): #runs in O(n^2)\n",
    "    \n",
    "     def createFeatures(self,data):\n",
    "        id_len=len(self.id) #we will add this length to the index if we want to access the second question. It is made clear in the implementation\n",
    "\n",
    "        features = dok_matrix((len(data),self.count*2)) #we will have twice the entries. One |VOC| x |2| one for the headline and the other one for the description\n",
    "        print(features.shape)\n",
    "        label = []\n",
    "        for i, row in data.iterrows():\n",
    "            for w in row['headline']:\n",
    "                if w in self.id:\n",
    "                    features[i,self.id[w]] += 1\n",
    "                    #features[i,self.id[w] + self.count] += 1\n",
    "            for w in row['short_description']:\n",
    "                if w in self.id:\n",
    "                    #features[i, self.id[w]]+= 1 #now there is one bow for the headline and another for the description\n",
    "                    features[i,self.id[w] + self.count] += 1\n",
    "            # print('i', i)\n",
    "            label.append(row['category encoded'])\n",
    "            \n",
    "            \n",
    "        #we want to make difference independent of the sentences,\n",
    "        #that is, no negative values. We must take absolute value of features\n",
    "        # not the most efficient thing ever here, but it does the job\n",
    "        \"\"\" i=0\n",
    "        for j, row in data.iterrows():\n",
    "            for w in row['headline']:\n",
    "                if w in self.id:\n",
    "                    if features[j,self.id[w]+self.count]<0:\n",
    "                        features[j,self.id[w]+self.count] *= -1\n",
    "            for w in row['short_description']:\n",
    "                if w in self.id:\n",
    "                     if features[j,self.id[w]+self.count]<0:\n",
    "                        features[j,self.id[w] + self.count] *= -1\n",
    "            i+=1.\n",
    "            if i%1000 == 0:\n",
    "                print('iteration: ', i)\"\"\"\n",
    "\n",
    "        #features=self.absoluteValue(features,data)\n",
    "        return features,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate size of the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "features=BowFeatureCreator()\n",
    "features.createFeatureSet(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features.id))\n",
    "print(len(features.voc))\n",
    "print(features.count)\n",
    "\n",
    "#print(train.shape)\n",
    "print(len(train))\n",
    "\n",
    "for k,v in features.id.items():\n",
    "    if v > len(features.id):\n",
    "        print(k)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f,l = features.createFeatures(train)\n",
    "print(features.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(f, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION\n",
    "\n",
    "We need to create many more variations with many different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "testf,testl = features.createFeatures(test)\n",
    "predicted_nb=mnb.predict(testf)\n",
    "print(classification_report(testl, predicted_nb));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE FROM HERE ONWARDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//we will trin the model once we are done with the cleaning\n",
    "\n",
    "First we split the data such that the proportion of category labels is the same in train and test. Remember, category will be our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, stratify=data['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave that for later, do not pay too much attention for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)))\n",
    "\n",
    "Then you use the class weights during the training process:\n",
    "\n",
    "train_history = model.fit( train_dataset, steps_per_epoch=n_steps, class_weight=class_weights )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1,'headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "for i, row in data.iterrows():\n",
    "    sentences.append(row['headline'])\n",
    "\n",
    "model_embeddings=Word2Vec(vector_size=20, min_count=1)\n",
    "model_embeddings.build_vocab(sentences, progress_per=10000)\n",
    "model_embeddings.train(sentences, total_examples=model_embeddings.corpus_count, epochs=30,report_delay=1)\n",
    "#model_embeddings.build(data['headlines'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings.wv.most_similar(positive=[\"korea\"])\n",
    "model_embeddings.wv.most_similar(positive=[\"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
